{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import cdist \n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from datetime import date, timedelta, datetime\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "def articles(ticker):\n",
    "    \n",
    "    print('Gathering Articles')\n",
    "    \n",
    "    def get_article_links(ticker):\n",
    "        # Set the URL for gathering articles about the ticker\n",
    "        url = f'https://seekingalpha.com/symbol/{ticker}/news'\n",
    "\n",
    "        # open the Selenium driver to the url\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        driver.implicitly_wait(50)\n",
    "        driver.get(url)\n",
    "        while True:\n",
    "            time.sleep(10)\n",
    "            if driver.find_elements_by_class_name(\"_1-r1S\"):\n",
    "                break            \n",
    "        links = driver.find_elements_by_class_name(\"_1-r1S\")\n",
    "        \n",
    "        # gather the links for the most recent articles\n",
    "        links = driver.find_elements_by_class_name(\"_1-r1S\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['date'] = [i.find_element_by_class_name(\"VbEwc\").text for i in links]\n",
    "        df['name'] = [i.find_element_by_tag_name(\"h3\").text for i in links]\n",
    "        df['href'] = [i.find_elements_by_tag_name('a')[0].get_property('href') for i in links]\n",
    "\n",
    "        # close the Seleium driver\n",
    "        driver.close()\n",
    "\n",
    "        # Clean it up to take out special characters from the article name\n",
    "        df['name'] = [re.sub('[^A-Za-z0-9 $]+', '', i) for i in df['name']]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_articles(ticker):\n",
    "\n",
    "        df = get_article_links(ticker)\n",
    "             \n",
    "        # Open the Selenium Driver\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        #start a list of the articles\n",
    "        art_list = []\n",
    "        # for loop to go through each of the links and grab the articles\n",
    "        for link in df['href']:\n",
    "            driver.get(link)       \n",
    "            while True:\n",
    "                try: \n",
    "                    driver.find_element_by_id(\"a-cont\")\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "                try: \n",
    "                    driver.find_element_by_class_name('premium-banner')\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "            try:\n",
    "                article = driver.find_element_by_id(\"a-cont\")\n",
    "            except:\n",
    "                article = 'error'\n",
    "            # The Article has comments, which for now I am going to take out. This code will separate the article into two sections with \"Like this article\" as the separator\n",
    "            sep = 'Recommended for you:'\n",
    "            try:\n",
    "                art_list.append(article.text.split(sep,1)[0])\n",
    "            except:\n",
    "                art_list.append('error')\n",
    "        df['article'] = art_list\n",
    "\n",
    "        # close the Seleium driver\n",
    "        driver.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    df = get_articles(ticker)\n",
    "\n",
    "\n",
    "    # This function will run through the list again and re-try error pages\n",
    "    def finish_qurom(df):\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        #start a list of the articles\n",
    "        art_list = []\n",
    "        # grap the comments while I'm here\n",
    "        comments = []\n",
    "        # for loop to go through each of the links and grab the articles\n",
    "        for r,c in df.iterrows():\n",
    "            if c[3] == 'error':\n",
    "                sep = 'Recommended for you:'\n",
    "                driver.get(c[2])\n",
    "                driver.implicitly_wait(50)\n",
    "                try:\n",
    "                    if driver.find_element_by_class_name('premium-banner'):\n",
    "                        df['article'][r] = 'premium-banner'\n",
    "                except:\n",
    "                    try:\n",
    "                        df['article'][r] = driver.find_element_by_id(\"a-cont\").text.split(sep,1)[0]\n",
    "                    except:\n",
    "                        article = 'error'\n",
    "                    try:\n",
    "                        df['comments'][r] = driver.find_element_by_id(\"a-cont\").text.split(sep,1)[1]\n",
    "                    except:\n",
    "                        comments.append('error')\n",
    "        driver.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    # run this new function to catch and sort errors\n",
    "#     df1 = finish_qurom(df)\n",
    "\n",
    "    df = df[df.article != 'premium-banner']\n",
    "\n",
    "\n",
    "    print(\"Articles gathered...vectorizing\")\n",
    "    # import google's word2vec located: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "    w2v_model = KeyedVectors.load_word2vec_format('../../Downloads/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "    # function for converting articles and article names into KeyedVectors\n",
    "\n",
    "    def clean2vect(articles):\n",
    "        new_list = []\n",
    "        for a in articles:\n",
    "            new_list.append(a.replace(',','').translate(string.punctuation))\n",
    "\n",
    "        clean_articles = [word.split() for word in new_list]\n",
    "\n",
    "        vects = []\n",
    "\n",
    "        for a in clean_articles:\n",
    "            vector = []\n",
    "            for word in a:\n",
    "                try:\n",
    "                    vector.append(w2v_model[word])\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                vects.append(sum(vector)/len(vector))\n",
    "            except:\n",
    "                vects.append(1)\n",
    "        return vects\n",
    "\n",
    "    # run both df columns through the clean2vect function which will append them to the df\n",
    "    df['article_vect'] = clean2vect(df['article'])\n",
    "    df['name_vect'] = clean2vect(df['name'])    \n",
    "    \n",
    "    print(\"Vectorized...seting datetime index\")\n",
    "\n",
    "    # clean up the dates from the articles to match the datetime format of the stock def\n",
    "    def art_date(df):\n",
    "        dates = []\n",
    "        for i in df.date:\n",
    "            if \"Today\" in i:\n",
    "                dates.append('2020-' + str(i.replace(i, date.today().strftime(\"%m-%d\"))))\n",
    "            elif 'Sat, Feb. 29' in i:\n",
    "                dates.append('2020-03-01')\n",
    "            elif \"Yesterday\" in i:\n",
    "                dates.append('2020-' + str(i.replace(i, (date.today() - timedelta(days = 1)).strftime(\"%m-%d\"))))\n",
    "            else:\n",
    "                try:\n",
    "                    dates.append('2020-' + str(datetime.strptime(i,'%a, %b. %d').strftime('%m-%d')))\n",
    "                except:\n",
    "                    try:\n",
    "                        dates.append('2020-' + str(datetime.strptime(i,'%a, %b %d').strftime('%m-%d')))\n",
    "                    except:\n",
    "                        dates.append(str(datetime.strptime(i,'%a, %b. %d, %Y').strftime('%Y-%m-%d')))\n",
    "        # set dataframe to new dates columns as index\n",
    "        df.date = dates\n",
    "        df.date = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    df = art_date(df)\n",
    "\n",
    "    print('Datetime Ready...Clustering...')\n",
    "\n",
    "    # Import pickle of trained KMeans Article Cluster Model\n",
    "    art_kluster_model = open('./pickle_jar/art_clusterer_pkl', 'rb')\n",
    "    art_kclusterer = pickle.load(art_kluster_model)\n",
    "    art_kluster_model.close()\n",
    "\n",
    "    # Import pickle of trained KMeans Name Cluster Model\n",
    "    name_kluster_model = open('./pickle_jar/name_clusterer_pkl', 'rb')\n",
    "    name_kclusterer = pickle.load(name_kluster_model)\n",
    "    name_kluster_model.close()\n",
    "\n",
    "\n",
    "    # assign clusters to new articles based on trained model\n",
    "    df['art_clusters'] = [art_kclusterer.classify_vectorspace(vector) for vector in df['article_vect']]\n",
    "    df['name_clusters'] = [name_kclusterer.classify_vectorspace(vector) for vector in df['article_vect']]\n",
    "\n",
    "    # pickle articles dataframe with new info before merging to X Features\n",
    "    with open('./pickle_jar/articles_predict', 'wb') as fp:\n",
    "        pickle.dump(df, fp)    \n",
    "\n",
    "    df.drop(['name','article','href', 'article_vect','name_vect'], axis=1, inplace=True)\n",
    "    \n",
    "    print('Getting Ticker Info')\n",
    "    \n",
    "    def get_stock(ticker):\n",
    "        stock = yf.Ticker(ticker).history()\n",
    "        stock.columns = [i.lower() for i in stock.columns]\n",
    "        stock = stock.pct_change()\n",
    "        stock[['dividends','stock splits']] = stock[['dividends','stock splits']].fillna(0)\n",
    "        stock.dropna(inplace=True)\n",
    "\n",
    "        return stock\n",
    "    \n",
    "    stock = get_stock(ticker)\n",
    "    \n",
    "    X = pd.merge(stock, df.groupby('date').mean(), left_index=True, right_index=True, how='right')\n",
    "    X.ffill(inplace=True)\n",
    "    X.fillna(2, inplace=True)\n",
    "    \n",
    "    # get the model scaler\n",
    "    dbfile = open('./pickle_jar/model_scaler', 'rb')      \n",
    "    sc = pickle.load(dbfile) \n",
    "    dbfile.close() \n",
    "    \n",
    "    #scale the new X and create dummy target column\n",
    "    X_new_sc = sc.transform(X)\n",
    "    y_dummy = np.zeros((X_new_sc.shape[0], ))\n",
    "\n",
    "    print('Modeling Predictions')\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_model('./pickle_jar/model_1.h5')\n",
    "    \n",
    "    # create new test sequencer for new data\n",
    "    new_test_seq = TimeseriesGenerator(X_new_sc\n",
    "                                   , y_dummy\n",
    "                                   , length=3\n",
    "                                   , batch_size=64)\n",
    "    #predict classes of new data on the trained model\n",
    "    answer = model.predict(new_test_seq)\n",
    "    # add predictions to dataframe\n",
    "    X['first'] = [0,0,0] + [1 if i[0][0] >= .5 else 0 for i in answer]\n",
    "    X['second'] = [0,0,0] + [1 if i[1][0] >= .5 else 0 for i in answer]\n",
    "    X['third'] = [0,0,0] + [1 if i[1][0] >= .5 else 0 for i in answer]\n",
    "    X['prediction'] = [(c[9] + c[10] + c[11]) for r,c in X.iterrows()]\n",
    "\n",
    "    with open('./pickle_jar/answer', 'wb') as fp:\n",
    "        pickle.dump(X, fp)    \n",
    "    \n",
    "    if X['prediction'][-1] == 3.0:\n",
    "        return f'Today we are predicting you should BUY {ticker} stock'\n",
    "    elif X['prediction'][-1] == 0.0:\n",
    "        return f'Today we are predicting you should SELL {ticker} stock'\n",
    "    else:\n",
    "        return f'Today we are predicting a HOLD for {ticker} stock'\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your ticker symbol:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " MSFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering Articles\n",
      "Articles gathered...vectorizing\n",
      "Vectorized...seting datetime index\n",
      "Datetime Ready...Clustering...\n",
      "Getting Ticker Info\n",
      "Modeling Predictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Today we are predicting you should BUY MSFT stock'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Enter your ticker symbol:\")\n",
    "ticker = input()\n",
    "stock = yf.Ticker(ticker).history()\n",
    "\n",
    "articles(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import article_gather\n",
    "articles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
